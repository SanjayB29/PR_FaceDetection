{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from typing import Tuple, List, Dict\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialFeatureExtractor:\n",
    "    def __init__(self, n_components_pca: int = 50, n_components_lda: int = None, use_lda: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the facial feature extractor.\n",
    "        \n",
    "        Args:\n",
    "            n_components_pca (int): Number of principal components to keep\n",
    "            n_components_lda (int): Number of LDA components (defaults to n_classes - 1)\n",
    "            use_lda (bool): Whether to apply LDA after PCA\n",
    "        \"\"\"\n",
    "        self.n_components_pca = n_components_pca\n",
    "        self.n_components_lda = n_components_lda\n",
    "        self.use_lda = use_lda\n",
    "        \n",
    "        # Initialize transformers\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_components_pca)\n",
    "        self.lda = LinearDiscriminantAnalysis(n_components=n_components_lda) if use_lda else None\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def load_image(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Load and verify image dimensions.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            Flattened image array\n",
    "        \"\"\"\n",
    "        # Load grayscale image\n",
    "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Verify dimensions\n",
    "        if img.shape != (112, 92):\n",
    "            raise ValueError(f\"Image {filepath} has incorrect dimensions. Expected (112, 92), got {img.shape}\")\n",
    "        \n",
    "        # Flatten the image to 1D array\n",
    "        return img.flatten()\n",
    "        \n",
    "    def extract_basic_features(self, img_array: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract basic statistical features from image.\n",
    "        \n",
    "        Args:\n",
    "            img_array: Flattened image array\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of basic features\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'mean_intensity': np.mean(img_array),\n",
    "            'std_intensity': np.std(img_array),\n",
    "            'median_intensity': np.median(img_array),\n",
    "            'min_intensity': np.min(img_array),\n",
    "            'max_intensity': np.max(img_array),\n",
    "            'range_intensity': np.ptp(img_array)\n",
    "        }\n",
    "        \n",
    "    def prepare_dataset(self, data_dir: str, csv_path: str) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Load images and prepare dataset for feature extraction.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Root directory containing image folders\n",
    "            csv_path: Path to the CSV file containing image metadata\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (image_data, labels, feature_names)\n",
    "        \"\"\"\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Initialize lists for data\n",
    "        image_data = []\n",
    "        labels = []\n",
    "        basic_features_list = []\n",
    "        \n",
    "        # Process each image\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                # Load and flatten image\n",
    "                img_array = self.load_image(row['filepath'])\n",
    "                \n",
    "                # Extract basic features\n",
    "                basic_features = self.extract_basic_features(img_array)\n",
    "                \n",
    "                # Store data\n",
    "                image_data.append(img_array)\n",
    "                labels.append(row['subject'])\n",
    "                basic_features_list.append(basic_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing {row['filepath']}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(image_data)\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        # Create feature names\n",
    "        feature_names = [f'pixel_{i}' for i in range(X.shape[1])]\n",
    "        feature_names.extend(basic_features_list[0].keys())\n",
    "        \n",
    "        # Combine pixel values with basic features\n",
    "        basic_features_array = np.array([list(f.values()) for f in basic_features_list])\n",
    "        X = np.hstack((X, basic_features_array))\n",
    "        \n",
    "        return X, y, feature_names\n",
    "        \n",
    "    def fit_transform(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Fit the feature extractors and transform the data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data matrix\n",
    "            y: Labels\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (transformed_features, feature_names)\n",
    "        \"\"\"\n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Generate PCA feature names\n",
    "        feature_names = [f'pca_component_{i}' for i in range(self.n_components_pca)]\n",
    "        \n",
    "        # Calculate explained variance ratio\n",
    "        explained_variance_ratio = self.pca.explained_variance_ratio_\n",
    "        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "        self.logger.info(f\"Cumulative explained variance ratio: {cumulative_variance_ratio[-1]:.3f}\")\n",
    "        \n",
    "        # Apply LDA if requested\n",
    "        if self.use_lda:\n",
    "            X_transformed = self.lda.fit_transform(X_pca, y)\n",
    "            feature_names = [f'lda_component_{i}' for i in range(X_transformed.shape[1])]\n",
    "        else:\n",
    "            X_transformed = X_pca\n",
    "        \n",
    "        return X_transformed, feature_names\n",
    "        \n",
    "    def save_features(self, features: np.ndarray, labels: np.ndarray, \n",
    "                     feature_names: List[str], output_path: str):\n",
    "        \"\"\"\n",
    "        Save extracted features to CSV file.\n",
    "        \n",
    "        Args:\n",
    "            features: Extracted feature matrix\n",
    "            labels: Subject labels\n",
    "            feature_names: Names of features\n",
    "            output_path: Path to save the CSV file\n",
    "        \"\"\"\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(features, columns=feature_names)\n",
    "        df['subject'] = labels\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(output_path, index=False)\n",
    "        self.logger.info(f\"Features saved to {output_path}\")\n",
    "        \n",
    "        # Log feature statistics\n",
    "        self.logger.info(f\"Dataset shape: {df.shape}\")\n",
    "        self.logger.info(f\"Number of subjects: {len(df['subject'].unique())}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to demonstrate feature extraction pipeline.\"\"\"\n",
    "    # Configuration\n",
    "    data_dir = 'dataset'  # Root directory containing image folders\n",
    "    csv_path = os.path.join(data_dir, 'data_log.csv')  # Path to metadata CSV\n",
    "    output_path = os.path.join(data_dir, 'facial_features.csv')  # Output path\n",
    "    \n",
    "    # Initialize feature extractor\n",
    "    extractor = FacialFeatureExtractor(\n",
    "        n_components_pca=50,  # Retain top 50 principal components\n",
    "        use_lda=True  # Apply LDA after PCA\n",
    "    )\n",
    "    \n",
    "    # Load and prepare dataset\n",
    "    X, y, initial_feature_names = extractor.prepare_dataset(data_dir, csv_path)\n",
    "    \n",
    "    # Extract features\n",
    "    features, feature_names = extractor.fit_transform(X, y)\n",
    "    \n",
    "    # Save features\n",
    "    extractor.save_features(features, y, feature_names, output_path)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
